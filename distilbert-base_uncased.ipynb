{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q transformers","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:08:41.603689Z","iopub.execute_input":"2023-07-17T18:08:41.605348Z","iopub.status.idle":"2023-07-17T18:08:55.942396Z","shell.execute_reply.started":"2023-07-17T18:08:41.605263Z","shell.execute_reply":"2023-07-17T18:08:55.940834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, requests\n\n\nDATA_DIR = \"PATH-YOU-WANT-TO-SAVE\"\nurl = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/'\n\nif not os.path.exists(DATA_DIR):\n    os.mkdir(DATA_DIR)\n\n    # loop through\n    for filename in ['train-v2.0.json', 'dev-v2.0.json']:\n        # make the request to download data over HTTP\n        res = requests.get(f'{url}{filename}')\n        # write to file\n        with open(f'{os.path.join(DATA_DIR, filename)}', 'wb') as f:\n            for chunk in res.iter_content(chunk_size=4):\n                f.write(chunk)\n\n        print(f\"{filename} downloaded.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:09:06.641337Z","iopub.execute_input":"2023-07-17T18:09:06.641898Z","iopub.status.idle":"2023-07-17T18:09:13.180447Z","shell.execute_reply.started":"2023-07-17T18:09:06.641835Z","shell.execute_reply":"2023-07-17T18:09:13.179130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modify if you have run above code\nDATA_DIR = \"../input/squad-20\"","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:09:23.153799Z","iopub.execute_input":"2023-07-17T18:09:23.155267Z","iopub.status.idle":"2023-07-17T18:09:23.161360Z","shell.execute_reply.started":"2023-07-17T18:09:23.155126Z","shell.execute_reply":"2023-07-17T18:09:23.160009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, json\n\n\ndef read_squad_json(filename: str) -> tuple:\n    \"\"\"\n    Give the datapath (representing train or dev set of SQuAD 2.0) and return the contexts, questions and answers\n    \"\"\"\n    path = os.path.join(DATA_DIR, filename)\n    with open(path, \"rb\") as json_file:\n        squad_dict = json.load(json_file)\n    \n    contexts, questions, answers = list(), list(), list()\n    \n    # # iterate through all data in squad data\n    for sample in squad_dict['data']:\n        for passage in sample['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                # check if we need to be extracting from 'answers' or 'plausible_answers'\n                access = \"plausible_answers\" if \"plausible_answers\" in qa.keys() else 'answers'\n                for answer in qa[access]:\n                    contexts.append(context)\n                    questions.append(question)\n                    answers.append(answer)\n    \n    return contexts, questions, answers\n\n\ntrain_contexts, train_questions, train_answers = read_squad_json('train-v2.0.json')\nvalid_contexts, valid_questions, valid_answers = read_squad_json('dev-v2.0.json')","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:09:26.163953Z","iopub.execute_input":"2023-07-17T18:09:26.164480Z","iopub.status.idle":"2023-07-17T18:09:28.422126Z","shell.execute_reply.started":"2023-07-17T18:09:26.164425Z","shell.execute_reply":"2023-07-17T18:09:28.420807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print some instances of training set. if you want see another batch of instances, change seed.\n\nfrom pprint import pprint\n\nimport random\nrandom.seed(0)\n\nindices = random.sample(range(0, len(train_contexts)), 5)\nfor index in indices:\n    print(f'Q:  {train_questions[index]}\\n')\n    print(\"Context:\\n\")\n    pprint(train_contexts[index])\n    print(f\"\\nAnswer:[{train_answers[index]}]\\n\")\n    print(\"-\" * 100)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:09:39.230705Z","iopub.execute_input":"2023-07-17T18:09:39.231150Z","iopub.status.idle":"2023-07-17T18:09:39.240370Z","shell.execute_reply.started":"2023-07-17T18:09:39.231111Z","shell.execute_reply":"2023-07-17T18:09:39.239387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_end_index(answers: list, contexts: list) -> list:\n    '''\n    the dataset has already character start_index of answers' \n    '''\n    _answers = answers.copy()\n    for answer, context in zip(_answers, contexts):\n        # this is the answer which is extracted from context \n        answer_bound = answer['text']\n        # we already know the start character position of answer from context\n        start_idx = answer['answer_start']\n        \n        answer['answer_end'] = start_idx + len(answer_bound)\n    return _answers\n\n\n\ntrain_answers = apply_end_index(train_answers, train_contexts)\nvalid_answers = apply_end_index(valid_answers, valid_contexts)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:09:47.912353Z","iopub.execute_input":"2023-07-17T18:09:47.913513Z","iopub.status.idle":"2023-07-17T18:09:47.998672Z","shell.execute_reply.started":"2023-07-17T18:09:47.913448Z","shell.execute_reply":"2023-07-17T18:09:47.997546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:09:55.125653Z","iopub.execute_input":"2023-07-17T18:09:55.126063Z","iopub.status.idle":"2023-07-17T18:09:57.934072Z","shell.execute_reply.started":"2023-07-17T18:09:55.126031Z","shell.execute_reply":"2023-07-17T18:09:57.932734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_data(contexts: list, questions: list, answers: list) -> dict:\n    encodings = tokenizer(contexts, questions, truncation=True, padding=True, return_tensors=\"pt\")\n\n    # add start and end positions to encodings\n    start_positions, end_positions = list(), list()\n\n    for index in range(len(answers)):\n        start_value = encodings.char_to_token(index, answers[index]['answer_start'])\n        end_value   = encodings.char_to_token(index, answers[index]['answer_end'])\n\n        # if start position is None, the answer passage has been truncated\n        if start_value is None:\n            start_value = tokenizer.model_max_length\n        \n        # end position cannot be found, char_to_token found space, so shift position until found\n        shift = 1\n        while end_value is None:\n            end_value = encodings.char_to_token(index, answers[index]['answer_end'] - shift)\n            shift += 1\n\n        start_positions.append(start_value)\n        end_positions.append(end_value)\n\n    encodings.update({\n        'start_positions': start_positions, 'end_positions': end_positions\n    })\n\n    return encodings\n\n\n\ntrain_encodings = encode_data(train_contexts, train_questions, train_answers)\nvalid_encodings = encode_data(valid_contexts, valid_questions, valid_answers)\n\ntrain_encodings.keys()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:10:19.072752Z","iopub.execute_input":"2023-07-17T18:10:19.080895Z","iopub.status.idle":"2023-07-17T18:12:08.216238Z","shell.execute_reply.started":"2023-07-17T18:10:19.080687Z","shell.execute_reply":"2023-07-17T18:12:08.214699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_contexts, train_questions, train_answers\ndel valid_contexts, valid_questions, valid_answers","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:12:18.580569Z","iopub.execute_input":"2023-07-17T18:12:18.581367Z","iopub.status.idle":"2023-07-17T18:12:18.636872Z","shell.execute_reply.started":"2023-07-17T18:12:18.581287Z","shell.execute_reply":"2023-07-17T18:12:18.635749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n\nclass SquadDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings: dict) -> None:\n        self.encodings = encodings\n\n    def __getitem__(self, index: int) -> dict:\n        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n\ntrain_ds = SquadDataset(train_encodings)\nvalid_ds = SquadDataset(valid_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:12:23.127652Z","iopub.execute_input":"2023-07-17T18:12:23.128096Z","iopub.status.idle":"2023-07-17T18:12:23.136991Z","shell.execute_reply.started":"2023-07-17T18:12:23.128057Z","shell.execute_reply":"2023-07-17T18:12:23.135512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_encodings, valid_encodings","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:12:37.962207Z","iopub.execute_input":"2023-07-17T18:12:37.962651Z","iopub.status.idle":"2023-07-17T18:12:37.968607Z","shell.execute_reply.started":"2023-07-17T18:12:37.962614Z","shell.execute_reply":"2023-07-17T18:12:37.967147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fine-tune the QuestionAnswering Transformer Model\nfrom transformers import AutoModelForQuestionAnswering\n\nmodel = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n\n# setup GPU/CPU\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# move model over to detected device\nmodel.to(device)\n# activate training mode of model\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:12:42.727286Z","iopub.execute_input":"2023-07-17T18:12:42.727770Z","iopub.status.idle":"2023-07-17T18:12:55.740381Z","shell.execute_reply.started":"2023-07-17T18:12:42.727723Z","shell.execute_reply":"2023-07-17T18:12:55.739421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nThis cell is adopted from `https://github.com/michaelrzhang/lookahead/blob/master/lookahead_pytorch.py`, which is the\nsource code of `Lookahead Optimizer: k steps forward, 1 step back` paper (https://arxiv.org/abs/1907.08610).\n\"\"\"\n\n\nfrom collections import defaultdict\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Lookahead(Optimizer):\n    r\"\"\"PyTorch implementation of the lookahead wrapper.\n    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n    \"\"\"\n\n    def __init__(self, optimizer, la_steps=5, la_alpha=0.8, pullback_momentum=\"none\"):\n        \"\"\"optimizer: inner optimizer\n        la_steps (int): number of lookahead steps\n        la_alpha (float): linear interpolation factor. 1.0 recovers the inner optimizer.\n        pullback_momentum (str): change to inner optimizer momentum on interpolation update\n        \"\"\"\n        self.optimizer = optimizer\n        self._la_step = 0  # counter for inner optimizer\n        self.la_alpha = la_alpha\n        self._total_la_steps = la_steps\n        pullback_momentum = pullback_momentum.lower()\n        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n        self.pullback_momentum = pullback_momentum\n\n        self.state = defaultdict(dict)\n\n        # Cache the current optimizer parameters\n        for group in optimizer.param_groups:\n            for p in group['params']:\n                param_state = self.state[p]\n                param_state['cached_params'] = torch.zeros_like(p.data)\n                param_state['cached_params'].copy_(p.data)\n                if self.pullback_momentum == \"pullback\":\n                    param_state['cached_mom'] = torch.zeros_like(p.data)\n\n    def __getstate__(self):\n        return {\n            'state': self.state,\n            'optimizer': self.optimizer,\n            'la_alpha': self.la_alpha,\n            '_la_step': self._la_step,\n            '_total_la_steps': self._total_la_steps,\n            'pullback_momentum': self.pullback_momentum\n        }\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def get_la_step(self):\n        return self._la_step\n\n    def state_dict(self):\n        return self.optimizer.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.optimizer.load_state_dict(state_dict)\n\n    def _backup_and_load_cache(self):\n        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n        \"\"\"\n        for group in self.optimizer.param_groups:\n            for p in group['params']:\n                param_state = self.state[p]\n                param_state['backup_params'] = torch.zeros_like(p.data)\n                param_state['backup_params'].copy_(p.data)\n                p.data.copy_(param_state['cached_params'])\n\n    def _clear_and_load_backup(self):\n        for group in self.optimizer.param_groups:\n            for p in group['params']:\n                param_state = self.state[p]\n                p.data.copy_(param_state['backup_params'])\n                del param_state['backup_params']\n\n    @property\n    def param_groups(self):\n        return self.optimizer.param_groups\n\n    def step(self, closure=None):\n        \"\"\"Performs a single Lookahead optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = self.optimizer.step(closure)\n        self._la_step += 1\n\n        if self._la_step >= self._total_la_steps:\n            self._la_step = 0\n            # Lookahead and cache the current optimizer parameters\n            for group in self.optimizer.param_groups:\n                for p in group['params']:\n                    param_state = self.state[p]\n                    p.data.mul_(self.la_alpha).add_(param_state['cached_params'], alpha=1.0 - self.la_alpha)  # crucial line\n                    param_state['cached_params'].copy_(p.data)\n                    if self.pullback_momentum == \"pullback\":\n                        internal_momentum = self.optimizer.state[p][\"momentum_buffer\"]\n                        self.optimizer.state[p][\"momentum_buffer\"] = internal_momentum.mul_(self.la_alpha).add_(\n                            1.0 - self.la_alpha, param_state[\"cached_mom\"])\n                        param_state[\"cached_mom\"] = self.optimizer.state[p][\"momentum_buffer\"]\n                    elif self.pullback_momentum == \"reset\":\n                        self.optimizer.state[p][\"momentum_buffer\"] = torch.zeros_like(p.data)\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:13:14.329305Z","iopub.execute_input":"2023-07-17T18:13:14.329807Z","iopub.status.idle":"2023-07-17T18:13:14.354159Z","shell.execute_reply.started":"2023-07-17T18:13:14.329759Z","shell.execute_reply":"2023-07-17T18:13:14.353143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\n\n# initialize adam optimizer with weight decay (reduces chance of overfitting)\nbase  = AdamW(model.parameters(), lr=1e-4)\noptim = Lookahead(base)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:13:42.870431Z","iopub.execute_input":"2023-07-17T18:13:42.870912Z","iopub.status.idle":"2023-07-17T18:13:42.965651Z","shell.execute_reply.started":"2023-07-17T18:13:42.870873Z","shell.execute_reply":"2023-07-17T18:13:42.964502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\n# initialize data loader for training data\ntrain_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n\n\nfor epoch in range(3):\n    # set model to train mode\n    model.train()\n    \n    # setup loop (we use tqdm for the progress bar)\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        # initialize calculated gradients (from prev step)\n        optim.zero_grad()\n        \n        # pull all the tensor batches required for training\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        \n        # train model on batch and return outputs (incl. loss)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n                        start_positions=start_positions, end_positions=end_positions)\n        # extract loss\n        loss = outputs[0]\n        # calculate loss for every parameter that needs grad update\n        loss.backward()\n        \n        # update parameters\n        optim.step()\n        \n        # print relevant info to progress bar\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:14:01.345398Z","iopub.execute_input":"2023-07-17T18:14:01.345954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_DIR = \"./model\"\n\nif not os.path.exists(MODEL_DIR):\n    os.mkdir(MODEL_DIR)\n\n\ntokenizer.save_pretrained(MODEL_DIR)\nmodel.save_pretrained(MODEL_DIR)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nMODEL_DIR = \"./model\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\nmodel = AutoModelForQuestionAnswering.from_pretrained(MODEL_DIR)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\n# switch model out of training mode\nmodel.eval()\nmodel = model.to(device)\n\n# initialize validation set data loader\nval_loader = DataLoader(valid_ds, batch_size=16)\n\n# initialize list to store accuracies\nacc = list()\n\n# loop through batches\nfor batch in val_loader:\n    # we don't need to calculate gradients as we're not training\n    with torch.no_grad():\n        # pull batched items from loader\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        # we will use true positions for accuracy calc\n        start_true = batch['start_positions'].to(device)\n        end_true = batch['end_positions'].to(device)\n        \n        # make predictions\n        outputs = model(input_ids, attention_mask=attention_mask)\n        # pull prediction tensors out and argmax to get predicted tokens\n        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n        \n        # calculate accuracy for both and append to accuracy list\n        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n        \n# calculate average accuracy in total\nprint(f\"Score of the model based on EM: {sum(acc)/len(acc)}\") ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def answer_to_questions(context: str, questions: list) -> list:\n    '''\n    return a list of answers to list of questions based on context.\n    '''\n    # encode the inputs\n    encodings = tokenizer([context]*len(questions), questions, truncation=True, padding=True, return_tensors=\"pt\")\n    encodings = encodings.to(device)\n    # make predictions\n    outputs = model(**encodings)\n    # pull prediction tensors out and argmax to get predicted tokens\n    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n    end_pred = torch.argmax(outputs['end_logits'], dim=1)\n    \n    answers = list()\n    for index, (start_idx, end_idx) in enumerate(zip(start_pred, end_pred)):\n        tokens = tokenizer.convert_ids_to_tokens(encodings['input_ids'][index][start_idx:end_idx+1])\n        answers.append( tokenizer.convert_tokens_to_string(tokens) )\n        \n        \n    # print the results\n    print(\"Context:\")\n    pprint(context)\n    print()\n    for question, answer in zip(questions, answers):\n        print(f\"Q:  {question}\")\n        print(f\"A:  {answer}\")\n        print(\"-\"*60)\n    \n    \n    \n    return answers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = \"The modern Olympic Games or Olympics (French: Jeux olympiques)[1][2] are leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of competitions. The Olympic Games are considered the world's foremost sports competition with more than 200 nations participating.[3] The Olympic Games are normally held every four years, alternating between the Summer and Winter Olympics every two years in the four-year period.\"\nquestions = [\n    \"How often do the Olympic games hold?\",\n    \"How many nations do participate in each Olympic?\"\n]\n\n_ = answer_to_questions(context, questions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = \"Vikings is the modern name given to seafaring people primarily from Scandinavia (present-day Denmark, Norway and Sweden), who from the late 8th to the late 11th centuries raided, pirated, traded and settled throughout parts of Europe. They also voyaged as far as the Mediterranean, North Africa, the Middle East, and North America. In some of the countries they raided and settled in, this period is popularly known as the Viking Age, and the term \\\"Viking\\\" also commonly includes the inhabitants of the Scandinavian homelands as a collective whole. The Vikings had a profound impact on the Early medieval history of Scandinavia, the British Isles, France, Estonia, and Kievan Rus'.\"\nquestions = [\n    \"When vikings started raided?\",\n]\n\n_ = answer_to_questions(context, questions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}