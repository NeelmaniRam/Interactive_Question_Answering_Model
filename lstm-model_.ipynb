{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\nimport numpy as np\n\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:21:17.961871Z","iopub.execute_input":"2023-07-25T19:21:17.962239Z","iopub.status.idle":"2023-07-25T19:21:51.407334Z","shell.execute_reply.started":"2023-07-25T19:21:17.962209Z","shell.execute_reply":"2023-07-25T19:21:51.406386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Load dataset**","metadata":{}},{"cell_type":"code","source":"def load_json(path):\n    '''\n    Loads the JSON file of the Squad dataset.\n    Returns the json object of the dataset.\n    '''\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n        \n    print(\"Length of data: \", len(data['data']))\n    print(\"Data Keys: \", data['data'][0].keys())\n    print(\"Title: \", data['data'][0]['title'])\n    \n    return data\n\ndef parse_data(data:dict)->list:\n    '''\n    Parses the JSON file of Squad dataset by looping through the\n    keys and values and returns a list of dictionaries with\n    context, query and label triplets being the keys of each dict.\n    '''\n    data = data['data']\n    qa_list = []\n\n    for paragraphs in data:\n\n        for para in paragraphs['paragraphs']:\n            context = para['context']\n\n            for qa in para['qas']:\n                \n                id = qa['id']\n                question = qa['question']\n                \n                for ans in qa['answers']:\n                    answer = ans['text']\n                    ans_start = ans['answer_start']\n                    ans_end = ans_start + len(answer)\n                    \n                    qa_dict = {}\n                    qa_dict['id'] = id\n                    qa_dict['context'] = context\n                    qa_dict['question'] = question\n                    qa_dict['label'] = [ans_start, ans_end]\n\n                    qa_dict['answer'] = answer\n                    qa_list.append(qa_dict)    \n\n    \n    return qa_list","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:21:58.480836Z","iopub.execute_input":"2023-07-25T19:21:58.481657Z","iopub.status.idle":"2023-07-25T19:21:58.495726Z","shell.execute_reply.started":"2023-07-25T19:21:58.481612Z","shell.execute_reply":"2023-07-25T19:21:58.494119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\ntrain_data = load_json('/kaggle/input/squad-20/dev-v2.0.json')\nvalid_data = load_json('/kaggle/input/squad-20/train-v2.0.json')\n\n# parse the json structure to return the data as a list of dictionaries\n\ntrain_list = parse_data(train_data)\nvalid_list = parse_data(valid_data)\nprint('--------------------------')\n\nprint('Train list len: ',len(train_list))\nprint('Valid list len: ',len(valid_list))\n\n# converting the lists into dataframes\n\ntrain_ds = pd.DataFrame(train_list)\nval_ds = pd.DataFrame(valid_list)\n\ntrain_ds = train_ds.drop('id', axis=1)\ntrain_ds = train_ds.drop('label', axis=1)\nval_ds = val_ds.drop('id', axis=1)\nval_ds = val_ds.drop('label', axis=1)\ntrain_ds.columns = ['Paragraph', 'Question', 'Answer']\nval_ds.columns = ['Paragraph', 'Question', 'Answer']","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:22:03.702716Z","iopub.execute_input":"2023-07-25T19:22:03.703108Z","iopub.status.idle":"2023-07-25T19:22:06.599115Z","shell.execute_reply.started":"2023-07-25T19:22:03.703078Z","shell.execute_reply":"2023-07-25T19:22:06.598002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:22:09.456122Z","iopub.execute_input":"2023-07-25T19:22:09.456654Z","iopub.status.idle":"2023-07-25T19:22:09.477224Z","shell.execute_reply.started":"2023-07-25T19:22:09.456609Z","shell.execute_reply":"2023-07-25T19:22:09.475969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:22:13.611952Z","iopub.execute_input":"2023-07-25T19:22:13.612465Z","iopub.status.idle":"2023-07-25T19:22:13.626054Z","shell.execute_reply.started":"2023-07-25T19:22:13.612422Z","shell.execute_reply":"2023-07-25T19:22:13.624918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Preprocessing","metadata":{}},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"%time train_ds['Paragraph'] = train_ds['Paragraph'].apply(nltk.word_tokenize)\n%time train_ds['Question'] = train_ds['Question'].apply(nltk.word_tokenize)\n%time train_ds['Answer'] = train_ds['Answer'].apply(nltk.word_tokenize)\n%time val_ds['Paragraph'] = val_ds['Paragraph'].apply(nltk.word_tokenize)\n%time val_ds['Question'] = val_ds['Question'].apply(nltk.word_tokenize)\n%time val_ds['Answer'] = val_ds['Answer'].apply(nltk.word_tokenize)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:22:17.964901Z","iopub.execute_input":"2023-07-25T19:22:17.965574Z","iopub.status.idle":"2023-07-25T19:26:12.365546Z","shell.execute_reply.started":"2023-07-25T19:22:17.965541Z","shell.execute_reply":"2023-07-25T19:26:12.364385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:27:37.702944Z","iopub.execute_input":"2023-07-25T19:27:37.703379Z","iopub.status.idle":"2023-07-25T19:27:37.729179Z","shell.execute_reply.started":"2023-07-25T19:27:37.703348Z","shell.execute_reply":"2023-07-25T19:27:37.727733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:27:40.258007Z","iopub.execute_input":"2023-07-25T19:27:40.258649Z","iopub.status.idle":"2023-07-25T19:27:40.281892Z","shell.execute_reply.started":"2023-07-25T19:27:40.258613Z","shell.execute_reply":"2023-07-25T19:27:40.280563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute maximum length statistics for paragraph and question\nparagraph_length = max(train_ds['Paragraph'].map(len).max(), val_ds['Paragraph'].map(len).max())\nquestion_length = max(train_ds['Question'].map(len).max(), val_ds['Question'].map(len).max())\nprint('Max paragraph length:', paragraph_length)\nprint('Max question length:', question_length)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:27:43.816022Z","iopub.execute_input":"2023-07-25T19:27:43.816415Z","iopub.status.idle":"2023-07-25T19:27:43.964246Z","shell.execute_reply.started":"2023-07-25T19:27:43.816385Z","shell.execute_reply":"2023-07-25T19:27:43.963101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode answers","metadata":{}},{"cell_type":"code","source":"num_not_found = 0\nnot_found = []\n\n# Map answer tokens to one-hot encodings of start and end positions of the answer span extracted from the paragraph\ndef encode_answer(paragraph_tokens, answer_tokens):\n    global num_not_found, not_found\n    answer_ptr = 0\n    start_pos = None\n    end_pos = None\n    for i, paragraph_token in enumerate(paragraph_tokens):\n        if paragraph_token == answer_tokens[answer_ptr]:\n            if start_pos == None:\n                start_pos = i\n            answer_ptr += 1\n            if answer_ptr == len(answer_tokens):\n                end_pos = i\n                break\n        elif start_pos != None:\n            start_pos = None\n            end_pos = None\n\n    start = [0] * paragraph_length\n    end = [0] * paragraph_length\n\n    if start_pos == None or end_pos == None:\n        num_not_found += 1\n        not_found.append([paragraph_tokens, answer_tokens])\n    else:\n        start[start_pos] = 1\n        end[end_pos] = 1\n    return [start, end]\n\ntrain_ds['Answer'] = train_ds.apply(lambda row: encode_answer(row['Paragraph'], row['Answer']), axis=1)\nval_ds['Answer'] = val_ds.apply(lambda row: encode_answer(row['Paragraph'], row['Answer']), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:27:55.618329Z","iopub.execute_input":"2023-07-25T19:27:55.619554Z","iopub.status.idle":"2023-07-25T19:28:03.682444Z","shell.execute_reply.started":"2023-07-25T19:27:55.619504Z","shell.execute_reply":"2023-07-25T19:28:03.681393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:28:11.409070Z","iopub.execute_input":"2023-07-25T19:28:11.409456Z","iopub.status.idle":"2023-07-25T19:28:11.447797Z","shell.execute_reply.started":"2023-07-25T19:28:11.409424Z","shell.execute_reply":"2023-07-25T19:28:11.446625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Embeddings - GloVe","metadata":{}},{"cell_type":"code","source":"embedding_file = 'glove.6B.50d.txt'\nembedding_size = 50\n\n# # Pre-computed unknown vector (by taking average of all word vectors)\n# # Reference: https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt\nunknown_vector = np.array([-0.12920076, -0.28866628, -0.01224866, -0.05676644, -0.20210965, -0.08389011,\n    0.33359843,  0.16045167,  0.03867431,  0.17833012,  0.04696583, -0.00285802,\n    0.29099807,  0.04613704,  -0.20923874, -0.06613114, -0.06822549, 0.07665912,\n    0.3134014,   0.17848536,  -0.1225775,  -0.09916984, -0.07495987, 0.06413227,\n    0.14441176,  0.60894334,  0.17463093,  0.05335403,  -0.01273871, 0.03474107,\n    -0.8123879,  -0.04688699, 0.20193407,  0.2031118,   -0.03935686, 0.06967544,\n    -0.01553638, -0.03405238, -0.06528071, 0.12250231,  0.13991883, -0.17446303,\n    -0.08011883, 0.0849521,   -0.01041659, -0.13705009, 0.20127155, 0.10069408,\n    0.00653003,  0.01685157], np.float32)\nprint(unknown_vector)\nprint(embedding_file)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:35:28.751705Z","iopub.execute_input":"2023-07-25T19:35:28.752156Z","iopub.status.idle":"2023-07-25T19:35:28.765610Z","shell.execute_reply.started":"2023-07-25T19:35:28.752121Z","shell.execute_reply":"2023-07-25T19:35:28.764543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport io\n\ndef get_glove_dict():\n    '''\n    Parses the glove word vectors text file and returns a dictionary with the words as\n    keys and their respective pretrained word vectors as values.\n    '''\n    encoding = 'utf-8'\n    glove_dict = {}\n    with open(\"/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\", \"r\") as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], \"float32\")\n            glove_dict[word] = vector\n\n    return glove_dict","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:37:50.236752Z","iopub.execute_input":"2023-07-25T19:37:50.237132Z","iopub.status.idle":"2023-07-25T19:37:50.244944Z","shell.execute_reply.started":"2023-07-25T19:37:50.237102Z","shell.execute_reply":"2023-07-25T19:37:50.243349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = get_glove_dict()\nprint(embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:37:53.497908Z","iopub.execute_input":"2023-07-25T19:37:53.498291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Assuming embeddings and unknown_vector are defined somewhere above this code\n\ndef embed(tokens):\n    vectors = []\n    for token in tokens:\n        token = token.lower()\n        if token in embeddings:\n            vectors.append(embeddings[token])\n        else:\n            vectors.append(unknown_vector)\n    return np.asarray(vectors, np.float32)\n\n# Assuming train_ds and val_ds are your training and validation datasets\n\n# Convert numpy arrays to lists before passing them to the embed function\ntrain_ds['Paragraph'], train_ds['Question'] = train_ds['Paragraph'].tolist(), train_ds['Question'].tolist()\nval_ds['Paragraph'], val_ds['Question'] = val_ds['Paragraph'].tolist(), val_ds['Question'].tolist()\n\ntrain_ds['Paragraph'], train_ds['Question'] = train_ds['Paragraph'].map(embed), train_ds['Question'].map(embed)\nval_ds['Paragraph'], val_ds['Question'] = val_ds['Paragraph'].map(embed), val_ds['Question'].map(embed)\n\n# If you want to store the vectors, create a variable for them\ntrain_paragraph_vectors = train_ds['Paragraph']\ntrain_question_vectors = train_ds['Question']\n\nval_paragraph_vectors = val_ds['Paragraph']\nval_question_vectors = val_ds['Question']\n\n# Now you can print the shape of the vectors\nprint(train_paragraph_vectors.shape)\nprint(train_question_vectors.shape)\nprint(val_paragraph_vectors.shape)\nprint(val_question_vectors.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:33:40.341295Z","iopub.execute_input":"2023-07-25T19:33:40.342625Z","iopub.status.idle":"2023-07-25T19:33:40.564792Z","shell.execute_reply.started":"2023-07-25T19:33:40.342585Z","shell.execute_reply":"2023-07-25T19:33:40.562927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform word tokens into word embeddings\ndef embed(tokens):\n    vectors = []\n    for token in tokens:\n        token = token.lower()\n        if token in embeddings:\n            vectors.append(embeddings[token])\n        else:\n            vectors.append(unknown_vector)\n    return np.asarray(vectors, np.float32)\n\ntrain_ds['Paragraph'], train_ds['Question'] = train_ds['Paragraph'].map(embed), train_ds['Question'].map(embed)\nval_ds['Paragraph'], val_ds['Question'] = val_ds['Paragraph'].map(embed), val_ds['Question'].map(embed)\nprint(vectors)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T19:29:07.307009Z","iopub.execute_input":"2023-07-25T19:29:07.307439Z","iopub.status.idle":"2023-07-25T19:29:29.275235Z","shell.execute_reply.started":"2023-07-25T19:29:07.307405Z","shell.execute_reply":"2023-07-25T19:29:29.273747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Prepare training and validation data","metadata":{}},{"cell_type":"code","source":"#print(vector)\n# Pad paragraph and question embeddings\ndef pad_paragraph(vectors):\n    remaining_length = paragraph_length - len(vectors)\n    paddings = np.repeat([np.zeros(embedding_size)], remaining_length, axis=0)\n    return np.concatenate((vectors, paddings), axis=0, dtype=np.float32)\n\ndef pad_question(vectors):\n    remaining_length = question_length - len(vectors)\n    paddings = np.repeat([np.zeros(embedding_size)], remaining_length, axis=0)\n    return np.concatenate((vectors, paddings), axis=0, dtype=np.float32)\n\nprint(vector)\nparagraph_train = train_ds['Paragraph'].map(pad_paragraph).to_list()\nquestion_train = train_ds['Question'].map(pad_question).to_list()\n\nparagraph_val = val_ds['Paragraph'].map(pad_paragraph).to_list()\nquestion_val = val_ds['Question'].map(pad_question).to_list()\nprint(pad_paragraph)\nprint(pad_question)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get start and end token positions\nanswer_train = train_ds['Answer'].to_list()\nstart_train = [ans[0] for ans in answer_train]\nend_train = [ans[1] for ans in answer_train]\n\nanswer_val = val_ds['Answer'].to_list()\nstart_val = [ans[0] for ans in answer_val]\nend_val = [ans[1] for ans in answer_val]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to constant tensor\nparagraph_train, question_train = tf.constant(paragraph_train, np.float32), tf.constant(question_train, np.float32)\nparagraph_val, question_val = tf.constant(paragraph_val, np.float32), tf.constant(question_val, np.float32)\nstart_train, end_train = tf.constant(start_train, np.float32), tf.constant(end_train, np.float32)\nstart_val, end_val = tf.constant(start_val, np.float32), tf.constant(end_val, np.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Feature Extraction - Bidirectional LSTM encoder","metadata":{}},{"cell_type":"code","source":"# LSTM encoder layer for paragraph\nparagraph_inputs = tf.keras.Input(shape=(paragraph_length, embedding_size))\nparagraph_inputs = tf.keras.layers.Masking(mask_value=np.zeros(embedding_size))(paragraph_inputs)\nparagraph_encoded = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_size, return_sequences=True))(paragraph_inputs)\n\n# LSTM encoder layer for question\nquestion_inputs = tf.keras.Input(shape=(question_length, embedding_size))\nquestion_inputs = tf.keras.layers.Masking(mask_value=np.zeros(embedding_size))(question_inputs)\nquestion_encoded = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_size, return_sequences=True))(question_inputs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Paragraph-Question Interaction - Bidirectional attention + One-hop interaction","metadata":{}},{"cell_type":"code","source":"# Co-attention layer\nscores = tf.matmul(paragraph_encoded, question_encoded, transpose_b=True)\nquestion_weights = tf.nn.softmax(scores)\nparagraph_weights = tf.nn.softmax(tf.transpose(scores, perm=[0,2,1]))\nquestion_context = tf.matmul(paragraph_encoded, question_weights, transpose_a=True)\nquestion_concat = tf.concat([tf.transpose(question_encoded, perm=[0,2,1]), question_context], axis=1)\nparagraph_context = tf.transpose(tf.matmul(question_concat, paragraph_weights), perm=[0,2,1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Span prediction - Unidirectional boundary model","metadata":{}},{"cell_type":"code","source":"# Answer pointer layer\nboundary = tf.keras.layers.LSTM(embedding_size, return_sequences=True)(paragraph_context)\nstart_pos_logits = tf.keras.layers.Dense(1)(boundary)\nboundary = tf.keras.layers.Concatenate()([boundary, start_pos_logits])\nend_pos_logits = tf.keras.layers.Dense(1)(boundary)\n\nstart_pos_distribution = tf.keras.layers.Softmax()(tf.squeeze(start_pos_logits, axis=-1))\nend_pos_distribution = tf.keras.layers.Softmax()(tf.squeeze(end_pos_logits, axis=-1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Build, fit and evaluate model","metadata":{}},{"cell_type":"code","source":"def cross_entropy_loss(y_true, y_pred):\n    start_true, end_true = y_true[0], y_true[1]\n    start_pred, end_pred = y_pred[0], y_pred[1]\n\n    start_loss = -tf.reduce_sum(start_true * tf.math.log(start_pred))\n    end_loss = -tf.reduce_sum(end_true * tf.math.log(end_pred))\n\n    total_loss = start_loss + end_loss\n    return total_loss\n\ndef exact_match(y_true, y_pred):\n    start_true, end_true = tf.math.argmax(y_true[0], output_type=tf.int32), tf.math.argmax(y_true[1], output_type=tf.int32)\n    start_pred = tf.math.argmax(y_pred[0], output_type=tf.int32)\n    end_pred = start_pred + tf.math.argmax(y_pred[1][start_pred:], output_type=tf.int32)\n\n    if start_pred != start_true or end_pred != end_true:\n        return float(0)\n    else:\n        return float(1)\n\ndef f1_score(y_true, y_pred):\n    start_true, end_true = tf.math.argmax(y_true[0], output_type=tf.int32), tf.math.argmax(y_true[1], output_type=tf.int32)\n    start_pred = tf.math.argmax(y_pred[0], output_type=tf.int32)\n    end_pred = start_pred + tf.math.argmax(y_pred[1][start_pred:], output_type=tf.int32)\n\n    start_min = tf.math.minimum(start_true, start_pred)\n    end_max = tf.math.maximum(end_true, end_pred)\n\n    true_pos = 0\n    false_neg = 0\n    false_pos = 0\n\n    for pos in range(start_min, end_max + 1):\n        in_true = start_true <= pos <= end_true\n        in_pred = start_pred <= pos <= end_pred\n\n        if in_true:\n            if in_pred:\n                true_pos += 1\n            else:\n                false_neg += 1\n        else:\n            if in_pred:\n                false_pos += 1\n\n    if true_pos == 0 and false_neg == 0 and false_pos == 0:\n        return float(0)\n\n    return float((2 * true_pos) / (2 * true_pos + false_pos + false_neg))\n\nmodel = tf.keras.Model(inputs=[paragraph_inputs, question_inputs], outputs =[start_pos_distribution, end_pos_distribution])\nmodel.compile(optimizer='adam', loss=cross_entropy_loss, metrics=[exact_match, f1_score])\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 4\nhistory = model.fit(\n    [paragraph_train, question_train],\n    [start_train, end_train],\n    epochs=epochs,\n    validation_data=([paragraph_val, question_val], [start_val, end_val])\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"em = history.history['softmax_1_exact_match']\nf1 = history.history['softmax_1_f1_score']\nloss = history.history['loss']\n\nval_em = history.history['val_softmax_1_exact_match']\nval_f1 = history.history['val_softmax_1_f1_score']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, em, label='Training EM')\nplt.plot(epochs_range, val_em, label='Validation EM')\nplt.legend(loc='lower right')\nplt.title('Bidirectional LSTM EM')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, f1, label='Training F1')\nplt.plot(epochs_range, val_f1, label='Validation F1')\nplt.legend(loc='lower right')\nplt.title('Bidirectional LSTM F1')\nplt.show()\n\nplt.subplot(2, 2, 3)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Bidirectional LSTM Loss')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = model.predict([paragraph_val, question_val])\npredictions = []\nfor start_dist, end_dist in zip(results[0], results[1]):\n    start_pred = np.argmax(start_dist)\n    end_pred = start_pred + np.argmax(end_dist[start_pred:])\n    predictions.append([start_pred, end_pred])\nwith open('model3_val.npy', 'wb') as f:\n    np.save(f, np.array(predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = model.predict([paragraph_train, question_train])\npredictions = []\nfor start_dist, end_dist in zip(results[0], results[1]):\n    start_pred = np.argmax(start_dist)\n    end_pred = start_pred + np.argmax(end_dist[start_pred:])\n    predictions.append([start_pred, end_pred])\nwith open('model3_train.npy', 'wb') as f:\n    np.save(f, np.array(predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/model3_val.npy', 'rb') as f:\n    model3_val = np.load(f)\n\nwith open('/kaggle/working/model3_train.npy', 'rb') as f:\n    model3_train = np.load(f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_testing = load_json('/kaggle/input/new-squaddataset/train-v2.0.json')\nvalid_data_testing = load_json('/kaggle/input/new-squaddataset/dev-v2.0.json')\n\n# parse the json structure to return the data as a list of dictionaries\n\ntrain_list_testing = parse_data(train_data_testing)\nvalid_list_testing = parse_data(valid_data_testing)\nprint('--------------------------')\n\nprint('Train list len: ',len(train_list_testing))\nprint('Valid list len: ',len(valid_list_testing))\n\n# converting the lists into dataframes\n\ntrain_ds_testing = pd.DataFrame(train_list_testing)\nval_ds_testing = pd.DataFrame(valid_list_testing)\n# train_ds_testing.columns = ['Paragraph', 'Question', 'Answer']\n# val_ds_testing.columns = ['Paragraph', 'Question', 'Answer']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds_testing.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = {}\nfor index, row in val_ds_testing.iterrows():\n    id_val = row['id']\n    paragraph = row['context']\n    question = row['question']\n    answer_start = row['label'][0]\n    answer = row['answer']\n    \n    answer_end = answer_start + len(answer)\n    if paragraph[answer_start-1:answer_end-1] == answer:\n        answer_start -= 1\n        answer_end -= 1\n    elif paragraph[answer_start-2:answer_end-2] == answer:\n        answer_start -= 2\n        answer_end -= 2\n\n    m3_start = model3_val[index][0]\n    m3_end = model3_val[index][1]\n    m3 = paragraph[m3_start:m3_end+1]\n    \n    predicted_answer = m3\n    actual_answer = answer\n\n    # Convert predicted and actual answers to sets of characters\n    predicted_chars = set(predicted_answer)\n    actual_chars = set(actual_answer)\n\n    # Calculate common characters\n    common_chars = predicted_chars.intersection(actual_chars)\n\n    # Calculate precision, recall, and F1 score at character level\n    precision = len(common_chars) / len(predicted_chars) if len(predicted_chars) > 0 else 0\n    recall = len(common_chars) / len(actual_chars) if len(actual_chars) > 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n    \n    predictions[id_val] = [m3,f1_score]\n\nprediction_validDataset_BiLSTM = 'prediction_validDataset_BiLSTM.txt'\nwith open(prediction_validDataset_BiLSTM, 'w', encoding='utf-8') as file:\n    json.dump(predictions, file, ensure_ascii=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = {}\nfor index, row in train_ds_testing.iterrows():\n    id_val = row['id']\n    paragraph = row['context']\n    question = row['question']\n    answer_start = row['label'][0]\n    answer = row['answer']\n    \n    answer_end = answer_start + len(answer)\n    if paragraph[answer_start-1:answer_end-1] == answer:\n        answer_start -= 1\n        answer_end -= 1\n    elif paragraph[answer_start-2:answer_end-2] == answer:\n        answer_start -= 2\n        answer_end -= 2\n\n    m3_start = model3_train[index][0]\n    m3_end = model3_train[index][1]\n    m3 = paragraph[m3_start:m3_end+1]\n    \n    predicted_answer = m3\n    actual_answer = answer\n\n    # Convert predicted and actual answers to sets of characters\n    predicted_chars = set(predicted_answer)\n    actual_chars = set(actual_answer)\n\n    # Calculate common characters\n    common_chars = predicted_chars.intersection(actual_chars)\n\n    # Calculate precision, recall, and F1 score at character level\n    precision = len(common_chars) / len(predicted_chars) if len(predicted_chars) > 0 else 0\n    recall = len(common_chars) / len(actual_chars) if len(actual_chars) > 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n    \n    predictions[id_val] = [m3,f1_score]\n\nprediction_trainDataset_BiLSTM = 'prediction_trainDataset_BiLSTM.txt'\nwith open(prediction_trainDataset_BiLSTM, 'w', encoding='utf-8') as file:\n    json.dump(predictions, file, ensure_ascii=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# F1 score\n","metadata":{}},{"cell_type":"code","source":"total_sum = 0\nnum_elements = 0\n\nfor key in predictions.keys():\n    total_sum += predictions[key][1]\n    num_elements += 1\n\nif num_elements > 0:\n    average = total_sum / num_elements\nelse:\n    average = 0\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(average)","metadata":{},"execution_count":null,"outputs":[]}]}