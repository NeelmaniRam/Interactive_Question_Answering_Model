{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n\n\n!pip install datasets\n!pip install transformers\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler\n\n#\"\"\"## Preprocessing Function \"\"\"\n\nimport json\nimport sys\nsys.stderr = open('temp.text', 'w')\n\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AdamW, BertForQuestionAnswering, BertTokenizer, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\ntokenizer = AutoTokenizer.from_pretrained('bert-large-cased')\nmodel = AutoModelForQuestionAnswering.from_pretrained('bert-large-cased')\n\ntraining_data = []\ndef preprocess_squad_dataset(dataset, tokenizer):\n    examples = []\n    \n    for entity in dataset['data']:\n        paras = entity['paragraphs']\n      \n        for in_entity in paras:\n            question_objects = in_entity['qas']\n            context = in_entity['context']\n            for in_in_entity in question_objects:\n                question = in_in_entity['question']\n                answers = in_in_entity['answers']\n                \n              \n                for ans in answers: \n                    ansrs = []\n                    start_positions = []\n                    end_positions = []\n                    start_index = ans['answer_start']\n                    answer = ans['text']\n                    tokid = 0\n                    for i in range(start_index+1):\n                        if(context[i] == ' '):\n                            tokid+=1\n                    ansrs.append(answer)\n                    # start_positions.append(tokid)\n                    # end_positions.append(len(answer.split(' ')) + tokid -1)\n                # Tokenize the input\n                    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, max_length=16, truncation=True, padding='max_length', return_tensors=\"pt\")\n\n                    input_ids = inputs['input_ids']\n                    token_type_ids = inputs['token_type_ids']\n                    attention_mask = inputs['attention_mask']\n                    start_position = tokid\n                    end_position = start_position + len(answer.split()) - 1\n\n                    examples.append({\n                        'input_ids': input_ids,\n                        'token_type_ids': token_type_ids,\n                        'attention_mask': attention_mask,\n                        'start_positions': torch.tensor([start_position]),\n                        'end_positions': torch.tensor([end_position])\n                    }) \n                    break\n                    \n            \n    return examples\n\n\"\"\"## Load dataset and preprocess it\"\"\"\n\nf = open(\"/content/train-v2.0.json\")\ndata = json.load(f)\npreprocessed_squad_dataset = preprocess_squad_dataset(data, tokenizer)\n\n\n\n\"\"\"## Train Dataset in batches\"\"\"\n\nnum_epochs = 20\n\nnum_training_steps = len(preprocessed_squad_dataset) * num_epochs\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\nmodel.train()\nsys.stderr = sys.__stderr__\n\nprint(\"started printin\")\nbatch_size = 32\n\nprint(\"started printin\")\n\nfor epoch in range(num_epochs):\n    for i in range(0, len(preprocessed_squad_dataset), batch_size):\n        batch = preprocessed_squad_dataset[i:i+batch_size]\n\n        input_ids = torch.stack([b['input_ids'][0] for b in batch])\n        token_type_ids = torch.stack([b['token_type_ids'][0] for b in batch])\n        attention_mask = torch.stack([b['attention_mask'][0] for b in batch])\n        start_positions = torch.stack([b['start_positions'][0] for b in batch])\n        end_positions = torch.stack([b['end_positions'][0] for b in batch])\n\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    print(epoch)\n\n\"\"\"## Save the model\"\"\"\n\ntorch.save(model.state_dict(), 'my_model_weights.pt')\n\n\"\"\"## Test on batches\"\"\"\n\nimport json \nbatch_size = 32\ntokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\ndef predict_answer(questions, contexts, model, tokenizer):\n    inputs = [question + tokenizer.sep_token + answer for question, answer in zip(questions, contexts)]\n\n    inputs = tokenizer.batch_encode_plus(inputs, add_special_tokens=True, max_length=16, truncation=True, padding='max_length', return_tensors=\"pt\")\n    input_ids = inputs['input_ids']\n    token_type_ids = inputs['token_type_ids']\n    attention_mask = inputs['attention_mask']\n\n    outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n\n    # Convert start_scores and end_scores to tensors\n    start_scores = outputs.start_logits.squeeze()\n    end_scores = outputs.end_logits.squeeze()\n\n\n    answers = []\n    for i in range(len(questions)):\n        # Get the most likely beginning and end of the answer\n        answer_start = torch.argmax(start_scores[i])\n        answer_end = torch.argmax(end_scores[i]) + 1\n        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[i][answer_start:answer_end]))\n        answer = answer.replace('[CLS]', '').replace('[SEP]', '').strip()\n        answers.append(answer)\n\n    return answers\n\nwith open('/content/dev-v2.0.json', 'r') as f:\n    test_data = json.load(f)\n\npredictions = {}\nbatch_questions = []\nbatch_contexts = []\nbatch_ids = []\nfor item in test_data['data']:\n    for para in item['paragraphs']:\n        context = para['context']\n        for question in para['qas']:\n            question_text = question['question']\n            question_id = question['id']\n            batch_questions.append(question_text)\n            batch_contexts.append(context)\n            batch_ids.append(question_id)\n            if len(batch_questions) == batch_size:\n                batch_answers = predict_answer(batch_questions, batch_contexts, model, tokenizer)\n                for i in range(len(batch_questions)):\n                     import json\nbatch_size = 32\ntokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\ndef predict_answer(questions, contexts, model, tokenizer):\n    inputs = [question + tokenizer.sep_token + answer for question, answer in zip(questions, contexts)]\n\n    inputs = tokenizer.batch_encode_plus(inputs, add_special_tokens=True, max_length=16, truncation=True, padding='max_length', return_tensors=\"pt\")\n    input_ids = inputs['input_ids']\n    token_type_ids = ['iâ€¦e_ids']\n    \n        for question in para['qas']:\n            question_text = question['question']\n            question_id = question['id']\n            batch_questions.append(question_text)\n            batch_contexts.append(context)\n            batch_ids.append(question_id)\n            if len(batch_questions) == batch_size:\n                batch_answers = predict_answer(batch_questions, batch_contexts, model, tokenizer)\n                for i in range(len(batch_questions)):\n                    predictions[batch_ids[i]] = batch_answers[i]\n                batch_questions = []\n                batch_contexts = []\n                batch_ids = []\n\nif len(batch_questions) > 0:\n    batch_answers = predict_answer(batch_questions, batch_contexts, model, tokenizer)\n    for i in range(len(batch_questions)):\n        predictions[batch_ids[i]] = batch_answers[i]\n\nwith open('predictions1.json', 'w') as f:\n    json.dump(predictions, f)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T06:20:06.207196Z","iopub.execute_input":"2023-07-20T06:20:06.207761Z","iopub.status.idle":"2023-07-20T06:20:06.271909Z","shell.execute_reply.started":"2023-07-20T06:20:06.207721Z","shell.execute_reply":"2023-07-20T06:20:06.270522Z"},"trusted":true},"execution_count":null,"outputs":[]}]}